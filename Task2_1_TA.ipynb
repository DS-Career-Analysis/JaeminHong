{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **텍스트 분석**\n",
    "\n",
    "- 비정형 데이터인 텍스트에서 의미 있는 정보를 추출\n",
    "\n",
    "- 머신러닝, 언어 이해, 통계 등을 활용하여 모델을 수립하고, 정보를 추출해 비즈니스 인텔리전스나 예측 분석 등의 분석 작업을 주로 수행\n",
    "\n",
    "- 머신러닝 알고리즘은 숫자형의 피처 기반 데이터만 입력받을 수 있기 때문에 텍스트를 머신러닝에 적용하기 위해선, 비정형 텍스트 데이터를 어떻게 피처 형태로 추출하고 추출된 피처에 의미 있는 값을 부여하는가가 매우 중요한 요소이다.\n",
    "\n",
    "- 피처 벡터화(Feature Vectorization) 또는 피처 추출(Feature Extraction)은 텍스트를 word 기반의 다수의 피처로 추출하고, 이 피처에 단어 빈도수와 같은 숫자 값을 부여하면 텍스트는 단어의 조합인 벡터값으로 표현될 수 있다.\n",
    "\n",
    "    - BOW(Bag of Words)\n",
    "\n",
    "    - Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **텍스트 분류 (Text Classification = Text Categorization)**\n",
    "\n",
    "- 문서가 특정 분류 또는 카테고리에 속하는 것을 예측\n",
    "\n",
    "- 특정 신문 기사 내용이 연예/정치/사회/문화 중 어떤 카테고리에 속하는지\n",
    "\n",
    "- 스팸 메일 검출\n",
    "\n",
    "- 지도학습\n",
    "\n",
    "## **감성 분석 (Sentiment Analysis)**\n",
    "\n",
    "- 텍스트에서 감정/판단/믿음/의견/기분 등의 주관적인 요소를 분석\n",
    "\n",
    "- 소셜 미디어 감정 분석, 영화나 제품에 대한 긍정 또는 리뷰, 여론조사 의견 분석\n",
    "\n",
    "- 지도학습, 비지도학습\n",
    "\n",
    "## **텍스트 요약 (Summarization)**\n",
    "\n",
    "- 텍스트 내에서 중요한 주제나 중심 사상을 추출\n",
    "\n",
    "- 토픽 모델링\n",
    "\n",
    "## **텍스트 군집화 (Clustering)**\n",
    "\n",
    "- 비슷한 유형의 문서에 대해 군집화\n",
    "\n",
    "- 텍스트 분류를 비지도학습으로 수행하는 방법의 일환으로 사용될 수 있음\n",
    "\n",
    "## **유사도 측정**\n",
    "\n",
    "- 문서들간의 유사도를 측정해 비슷한 문서끼리 모을 수 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **텍스트 분석 수행 프로세스**\n",
    "\n",
    "- 텍스트 사전 준비작업(텍스트 전처리) = 텍스트 정규화\n",
    "\n",
    "    - 클렌징 작업 : 대/소문자 변경, 특수문자 삭제, ...\n",
    "\n",
    "    - 토큰화 작업\n",
    "\n",
    "    - 의미 없는 단어 (Stop word) 제거 작업\n",
    "\n",
    "    - 어근 추출(Stemming/Lemmatization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **클렌징**\n",
    "\n",
    "- 불필요한 문자, 기호 등을 사전에 제거\n",
    "\n",
    "- HTML, XML, 태그나 특정 기호"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **텍스트 토큰화**\n",
    "\n",
    "- 문서에서 문장을 분리하는 문장 토큰화\n",
    "\n",
    "- 문장에서 단어를 토큰으로 분리하는 단어 토큰화\n",
    "\n",
    "- NLTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **문장 토큰화**\n",
    "- 문장의 마침표(.), 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['가나 다라 마바사, 아자 차카 타파하.', 'ABC DEFG, HIJK LMNOP.', 'QRS TUV, W XYZ.']\n"
     ]
    }
   ],
   "source": [
    "text_sample = '가나 다라 마바사, 아자 차카 타파하.\\nABC DEFG, HIJK LMNOP.\\nQRS TUV, W XYZ.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **단어 토큰화**\n",
    "- 공백, 콤마(,), 마침표(.), 개행문자 등으로 분리하지만, 정규 표현식을 이용해 다양한 유형으로 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 20\n",
      "['가나', '다라', '마바사', ',', '아자', '차카', '타파하', '.', 'ABC', 'DEFG', ',', 'HIJK', 'LMNOP', '.', 'QRS', 'TUV', ',', 'W', 'XYZ', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentences = '가나 다라 마바사, 아자 차카 타파하.\\nABC DEFG, HIJK LMNOP.\\nQRS TUV, W XYZ.'\n",
    "words = word_tokenize(sentences)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모든 단어에 대해서 토큰화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['가나', '다라', '마바사', ',', '아자', '차카', '타파하', '.'], ['ABC', 'DEFG', ',', 'HIJK', 'LMNOP', '.'], ['QRS', 'TUV', ',', 'W', 'XYZ', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "\n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stop word 제거**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words 개수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('stop words 개수:', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8c8bec5cc0c362bf9eee36b36fc26918edda74392b07a5cfd5fc0a1c1ef43da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
